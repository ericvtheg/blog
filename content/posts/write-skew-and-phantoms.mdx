---
title: Database Transaction are a Leaky Abstraction
description: Transactions are empowering for application developers, but the conversation doesn't stop there.
date: "2023-08-15"
---

As application developers, database transactions abstract us away from
[concurrency control](https://www.geeksforgeeks.org/concurrency-control-in-dbms/#)
to let us design our systems as if just a single process is modifying state serially. [1] {/* 233 */}
Unfortunately it is not that simple.

Disclaimer: This article is influenced by Martin Kleppmann's book, _Designing Data-Intensive Applications_.

<p align="center">
  <a
    alt="Designing Data-Intensive Applications by Martin Kleppmann"
    target="_blank"
    href="https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321/ref=sr_1_1?keywords=martin+kleppmann%2527s+designing+data-intensive+applications&amp;qid=1691551612&amp;sprefix=martin+klep%252Caps%252C192&amp;sr=8-1&_encoding=UTF8&tag=ericventor-20&linkCode=ur2&linkId=e1be69645fddf1c77e3edaeafb9d9c2a&camp=1789&creative=9325"
  >
    <img
      src="https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSlR4hC5tiypE9ftFBwNG1qi90tsMqcjs3cZB1vZrk7qZ5YV3dt"
      alt="Book Cover"
    />
    Designing Data-Intensive Applications by Martin Kleppmann
  </a>
</p>

## Transactions & Concurrency Control

Database transactions give us the illusion that all operations are done serially.
If all our database operations were done serially we wouldn't need to worry about our
data being manipulated in ways we didn't anticipate.

This illusion is done through concurrency control via transaction isolation.
Transaction isolation can have detrimental _gotchas_ that we must be aware of.
In order to build robust systems we must thoroughly understand how transactions
work and what level of isolation our database provides.

## Transaction Isolation Levels

Transaction isolation refers to how the database itself handles data access.
Each isolation level has its own rules for when data can be read/written to.
Limiting access to other transactions is done through locking and serialization.
As data access becomes more and more strict there are trade-offs, we get stronger
data consistency, but weaker concurrency capacity.

Understanding how our databases writes and reads is crucial for mitigating the potential
of race conditions. To add onto the complexity there is no standard for isolation levels.
For example Snapshot isolation in PostgreSQL is called repetable read, while Oracle
calls it serializable. [pg 242]

Before diving into different transaction isolation levels, lets discuss
what we are trying to avoid.

## Race Conditions

Race conditions can occur when multiple processes attempt to modify data at the same time
leading to situations where unexpected outcomes occur.

An example of a race condition can be as benign as a user creating an account
but not seeing their profile when they refresh the page.
[A race condition can also be as sinister as bankrupting a CryptoConcurrency exchange](https://old.reddit.com/r/Bitcoin/comments/1wtbiu/how_i_stole_roughly_100_btc_from_an_exchange_and/).

Race conditions are tricky to test for because they only occur when we get unlucky with timing. [2] {/* 233 */}
To understand the depth of what in our software stack can be 'unlucky' we must recognize
that anomalies can occur anywhere from the top of the software stack all the way to the
hardware at the bottom. For the purpose of this article we will focus on the race conditions
that can occur in our database and how we can mitigate them using different transaction isolation levels.

### Mitigating Race Conditions

In this section we will go through race condition examples and discuss how they could be avoided
with different transation isolation levels.

### Read Committed

The most basic level of transaction isolation is read committed. It makes two guarantees:

> 1. When reading from the database, you will only see data that has been committed (no dirty reads).
> 2. When writing to the database, you will overwrite data that has been commited (no dirty writes).

<cite>&mdash; Martin Kleppmann</cite> [pg 234]

#### Race Condition Example

The race conditions prevented by read committed isolation level are fundamental to database integrity.
A fundamental example is maintaining state between multiple rows within a schema.

For this exmaple a user has bank accounts X and Y. They decide to transfer $10,000 from X and deposit into Y.
Since this entails two separate operations, this would require a database transaction
to ensure integrity in the operation. The result of the transaction should not be exposed
until it is fully completed. If this wasn't the case imagine that $10,000 was the total amount of money
the user had between bank accounts X and Y, in the middle of this operation executing the user refreshes the page.
Thus causing the user to see $0 between both bank accounts, a very uncomftorable situation for the user to be in.

Another case for this example could be the bank is determining how much to charge for the user's bank
accountfor the month. The bank has a policy where if the total dollars exceeds $5,000 then there is no
monthly bill. Unfortunately due to the lack of a transaction, the monthly billing service read $0 in
both accounts, causing a charge. A direct effect of the race condition with real world consequences.

### Snapshot Isolation and Repeatable Read

As the name implies, in the snapshot isolation transaction level, all transactions work on a snapshot
of the database from when the transaction began. Regardless if subsequent transactions commit, the
data that is being read/written will stay the same from the perspective of the transaction. [pg 232]

Snapshot isolation is used in PostgreSQL and MySQL and is referred to as repeatable read. [pg 242]

#### Race Condition Example - Read Skew

An excellent example of a race condition that is avoided through the use of snapshot isolation is
creating a database backup. Naturally when creating a backup of potentially several terabytes of data,
it will take a relatively long amount of time. Therefore especially exposing us to race conditions.

As the backup is created several other transactions can complete, potentially exposing us to
inconsistencies in our data. [pg 238] If a user's bank account X was written to the backup with
$500, but later there was a transfer from bank account Y (that has not yet been written to the backup)
that sent $100 to bank account X. Then there will be an inconsistency in the backup
where $100 went missing. This inconsistency is called read skew. With snapshot isolation for
the entire length of the backup creation process the database will remain consistent, allowing us
to create a reliable database backup.

### Serializable Isolation

We started this article saying that transaction isolation is a means to prevent data abnormalities
from concurrency. What we still allowed concurrency but every transaction is executed as if it had
executed one at a time. This level of transaction isolation is known as serialization. [pg 252]

There are different ways to implement this either by literally executing a transaction at a time
(often with partitioned data) or with extensive locking.

It is worth noting that the tradeoff for serialization isolation level are significant. While
data integrity is guaranteed, there is a lack of scaling [pg. 256] and the potential for much
higher read/write times. [pg 259 top of page]

#### Race Condition Example - Write Skew

Write skew is a more sly example of a race condition. This is due to it not being
able to be prevented using the previous transaction isolation levels mentioned.
Write skew is caused by phantoms, in other words, where we check for the abscence of data.

For example, I go to a fitness studio with live classes called Orange Theory
(not affiliated but I am a fan). I attend Orange Theory classes by going into the app
and booking a class. Naturally a class has a finite amount of capacity that
can not be exceeded, so when scheduling a class for myself the database insert probably
looks something like:

```sql
INSERT Eric INTO SCHEDULED_ATTENDEES
WHERE SCHEDULED_ATTENDEES.class_id = X
AND 20 > (SELECT COUNT(*) FROM SCHEDULED_ATTENDEES WHERE class_id = X)
```

Excuse the crude example, but as you can see we're checking that there is less than
20 people scheduled to attend class X. In other words we're checking for the absence of rows.

What happens if we only have a single slot left for a class, and two users request to book at
the same time? At the time of each request's database query there is less than 20 users at the
booked, so the database does the insert and the request is successful. Now we have 21 users booked
for a class that only has capacity for 20 people. Even in a transaction, there is no failure to
cause a rollback in this case. The database schema itself has no constraint on the amount of
scheduled attendees for a class.

This is solved if we literally execute each query serially on a single process, but what about
the serialization isolation implemented using locks? The exact same case would **still** occur.
So how do we solve this? We use predicate locking. [pg 259] Predicate locks work by locking
a whole range of rows that are accessed in the query, whether they exist or not. By issueing a lock
on the range of rows in this case, we prevent the race condition thus allowing us to safely book
our live fitness studio class. Phew.

## Conclusion

this article doesn't even discuss replication which adds network, balancing, consensus, etc to the mix.

We've gone through various levels of transaction isolation, so which do we use? Trade offs.

In the end there is no one size fits all solution, and the best we can do is understand how our database
works and build arround it in our application. (leaky abstraction)

As much as we'd like to trust our abstractions and lean into them, we are forced to think about every layer's failure in our systems
and account for them.
